#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TRIGNUM-300M / T-CHIP â€” CopilotPipeline.py                  â•‘
â•‘  End-to-end automation: fetch â†’ filter â†’ log â†’ visualize     â•‘
â•‘                                                               â•‘
â•‘  Usage:  python CopilotPipeline.py                           â•‘
â•‘  Output: Console report + benchmarks/copilot_results.json    â•‘
â•‘          + benchmarks/copilot_dashboard.png (if matplotlib)  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json
import os
import sys
import time
import urllib.request
from collections import defaultdict
from dataclasses import asdict, dataclass
from typing import List, Optional

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))
from trignum_core.subtractive_filter import SubtractiveFilter

HALUEVAL_DIR = os.path.join(os.path.dirname(__file__), "benchmarks", "halueval_data")
RESULTS_DIR  = os.path.join(os.path.dirname(__file__), "benchmarks")

# â”€â”€ Optional matplotlib â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.patches as mpatches
    import matplotlib.pyplot as plt
    import numpy as np
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False


# â”€â”€â”€ DATA STRUCTURES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class Sample:
    text: str
    has_hallucination: bool
    task: str
    db: str

@dataclass
class SampleResult:
    sample: Sample
    predicted: bool
    illogics: List[str]
    subtraction_ratio: float
    latency_ms: float

    @property
    def tp(self): return self.predicted and self.sample.has_hallucination
    @property
    def fp(self): return self.predicted and not self.sample.has_hallucination
    @property
    def tn(self): return not self.predicted and not self.sample.has_hallucination
    @property
    def fn(self): return not self.predicted and self.sample.has_hallucination


# â”€â”€â”€ DATA LOADERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_halueval_jsonl(filename: str, right_key: str, halluc_key: str,
                        task: str, db: str, limit: int = 0) -> List[Sample]:
    path = os.path.join(HALUEVAL_DIR, filename)
    if not os.path.exists(path):
        print(f"  âš   Not found: {filename}")
        return []
    samples = []
    with open(path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if limit and i >= limit:
                break
            line = line.strip()
            if not line:
                continue
            try:
                row = json.loads(line)
            except json.JSONDecodeError:
                continue
            right  = row.get(right_key, "").strip()
            halluc = row.get(halluc_key, "").strip()
            if right:
                samples.append(Sample(text=right,  has_hallucination=False, task=task, db=db))
            if halluc:
                samples.append(Sample(text=halluc, has_hallucination=True,  task=task, db=db))
    return samples


def fetch_hf_api(dataset: str, config: str, split: str,
                 limit: int = 200) -> List[dict]:
    url = (
        f"https://datasets-server.huggingface.co/rows"
        f"?dataset={dataset}&config={config}&split={split}"
        f"&offset=0&length={limit}"
    )
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=15) as r:
            data = json.loads(r.read().decode("utf-8"))
        return [row["row"] for row in data.get("rows", [])]
    except Exception as e:
        print(f"  âš   HuggingFace fetch failed ({dataset}): {e}")
        return []


def load_truthfulqa(limit: int = 200) -> List[Sample]:
    rows = fetch_hf_api("truthfulqa%2Ftruthful_qa", "generation", "validation", limit=limit // 2)
    samples = []
    for row in rows:
        q    = row.get("question", "")
        best = row.get("best_answer", "").strip()
        bad  = row.get("incorrect_answers", [])
        if best:
            samples.append(Sample(f"Q: {q}\nA: {best}", False, "truthful_qa", "[F] TruthfulQA"))
        if bad:
            samples.append(Sample(f"Q: {q}\nA: {bad[0]}", True, "truthful_qa", "[F] TruthfulQA"))
    return samples


def load_medhallu(limit: int = 60) -> List[Sample]:
    rows = fetch_hf_api("medalpaca%2Fmedical_meadow_medical_flashcards", "default", "train", limit=limit)
    return [
        Sample(
            text=f"Topic: {r.get('input','')}\nDetail: {r.get('output','')}",
            has_hallucination=False,
            task="medical_clean",
            db="[G] MedHallu Proxy"
        )
        for r in rows if r.get("output")
    ]


def load_all_datasets(halueval_limit: int = 2000,
                      tqa_limit: int = 200,
                      med_limit: int = 60) -> List[Sample]:
    """Load all available benchmark datasets."""
    print("\n  â”€â”€ LOADING DATASETS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    all_samples: List[Sample] = []

    sets = [
        ("qa_data.json",            "right_answer",   "hallucinated_answer",  "qa_factual",      "[B] HaluEval QA"),
        ("dialogue_data.json",      "right_response", "hallucinated_response","dialogue_factual", "[C] HaluEval Dialogue"),
        ("summarization_data.json", "right_summary",  "hallucinated_summary", "summary_factual", "[D] HaluEval Summarization"),
        ("general_data.json",       "right_answer",   "hallucinated_answer",  "general_factual", "[E] HaluEval General"),
    ]
    for fname, rk, hk, task, db in sets:
        s = load_halueval_jsonl(fname, rk, hk, task, db, limit=halueval_limit)
        if s:
            print(f"  âœ“ {db}: {len(s):,} samples")
            all_samples.extend(s)

    print(f"  ğŸŒ Fetching TruthfulQA...", end=" ", flush=True)
    tqa = load_truthfulqa(limit=tqa_limit)
    print(f"{len(tqa)} samples" if tqa else "failed")
    all_samples.extend(tqa)

    print(f"  ğŸŒ Fetching MedHallu Proxy...", end=" ", flush=True)
    med = load_medhallu(limit=med_limit)
    print(f"{len(med)} samples" if med else "failed")
    all_samples.extend(med)

    print(f"\n  Total loaded: {len(all_samples):,} samples")
    return all_samples


# â”€â”€â”€ EVALUATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def evaluate(samples: List[Sample], sf: SubtractiveFilter) -> List[SampleResult]:
    results = []
    for s in samples:
        if not s.text or len(s.text.strip()) < 5:
            continue
        t0     = time.perf_counter()
        res    = sf.apply(s.text)
        lat_ms = (time.perf_counter() - t0) * 1000
        pred   = len(res.illogics_found) > 0 and res.subtraction_ratio > 0
        results.append(SampleResult(
            sample=s,
            predicted=pred,
            illogics=res.illogics_found,
            subtraction_ratio=res.subtraction_ratio,
            latency_ms=lat_ms,
        ))
    return results


def compute_metrics(results: List[SampleResult]) -> dict:
    tp = sum(1 for r in results if r.tp)
    fp = sum(1 for r in results if r.fp)
    tn = sum(1 for r in results if r.tn)
    fn = sum(1 for r in results if r.fn)
    n  = max(tp + fp + tn + fn, 1)
    prec   = tp / max(tp + fp, 1)
    recall = tp / max(tp + fn, 1)
    f1     = 2 * prec * recall / max(prec + recall, 1e-9)
    acc    = (tp + tn) / n
    lats   = [r.latency_ms for r in results]
    illogic_counts = defaultdict(int)
    for r in results:
        for illogic in r.illogics:
            illogic_counts[illogic.split(":")[0]] += 1
    return dict(
        total=n, tp=tp, fp=fp, tn=tn, fn=fn,
        precision=prec, recall=recall, f1=f1, accuracy=acc,
        avg_latency_ms=sum(lats)/max(len(lats),1),
        max_latency_ms=max(lats) if lats else 0,
        throughput_per_sec=n / max(sum(lats)/1000, 1e-9),
        illogic_distribution=dict(illogic_counts),
    )


# â”€â”€â”€ VISUALIZATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_dashboard(all_results: List[SampleResult],
                       db_metrics: dict,
                       aggregate: dict,
                       output_path: str):
    """Generate a 6-panel metrics dashboard PNG."""
    if not MATPLOTLIB_AVAILABLE:
        print("  âš   matplotlib not installed â€” skipping dashboard. pip install matplotlib")
        return

    dbs    = list(db_metrics.keys())
    colors = plt.cm.get_cmap("tab10")

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.patch.set_facecolor("#0d1117")
    for ax in axes.flat:
        ax.set_facecolor("#161b22")
        ax.tick_params(colors="white")
        ax.spines[:].set_color("#30363d")
        ax.title.set_color("white")
        ax.xaxis.label.set_color("white")
        ax.yaxis.label.set_color("white")

    # 1 â€” Per-dataset accuracy bar chart
    ax = axes[0, 0]
    accs  = [db_metrics[db]["accuracy"] for db in dbs]
    short = [db.split("]")[-1].strip() for db in dbs]
    bars  = ax.bar(short, accs, color=[colors(i) for i in range(len(dbs))])
    ax.set_ylim(0, 1.1)
    ax.set_title("Per-Dataset Accuracy", fontweight="bold")
    ax.set_ylabel("Accuracy")
    ax.axhline(0.9, color="#58a6ff", linestyle="--", linewidth=1, label="Target 90%")
    ax.legend(facecolor="#161b22", labelcolor="white", fontsize=8)
    plt.setp(ax.get_xticklabels(), rotation=30, ha="right", fontsize=7)
    for bar, val in zip(bars, accs):
        ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, f"{val:.1%}",
                ha="center", color="white", fontsize=7)

    # 2 â€” Precision / Recall / F1 grouped bar
    ax = axes[0, 1]
    x   = range(len(dbs))
    w   = 0.25
    ax.bar([i - w for i in x], [db_metrics[db]["precision"] for db in dbs], w, label="Precision", color="#58a6ff")
    ax.bar(list(x),             [db_metrics[db]["recall"]   for db in dbs], w, label="Recall",    color="#3fb950")
    ax.bar([i + w for i in x], [db_metrics[db]["f1"]       for db in dbs], w, label="F1",        color="#f78166")
    ax.set_xticks(list(x)); ax.set_xticklabels(short, rotation=30, ha="right", fontsize=7)
    ax.set_ylim(0, 1.1); ax.set_title("Precision / Recall / F1", fontweight="bold")
    ax.legend(facecolor="#161b22", labelcolor="white", fontsize=8)

    # 3 â€” Confusion matrix (aggregate)
    ax = axes[0, 2]
    cm = np.array([[aggregate["tp"], aggregate["fn"]],
                   [aggregate["fp"], aggregate["tn"]]])
    im = ax.imshow(cm, cmap="Blues")
    for (i, j), v in np.ndenumerate(cm):
        ax.text(j, i, f"{v:,}", ha="center", va="center",
                color="white" if v > cm.max()/2 else "#161b22", fontsize=10, fontweight="bold")
    ax.set_xticks([0,1]); ax.set_yticks([0,1])
    ax.set_xticklabels(["Pred: Halluc", "Pred: Clean"])
    ax.set_yticklabels(["Act: Halluc", "Act: Clean"])
    ax.set_title(f"Aggregate Confusion Matrix\n({aggregate['total']:,} samples)", fontweight="bold")
    plt.colorbar(im, ax=ax)

    # 4 â€” Latency histogram
    ax = axes[1, 0]
    lats = [r.latency_ms for r in all_results if r.latency_ms < 5]
    ax.hist(lats, bins=50, color="#58a6ff", edgecolor="#0d1117", alpha=0.85)
    ax.set_title("Latency per Sample (< 5ms)", fontweight="bold")
    ax.set_xlabel("ms"); ax.set_ylabel("Count")
    ax.axvline(aggregate["avg_latency_ms"], color="#f78166", linestyle="--",
               label=f"Avg {aggregate['avg_latency_ms']:.3f}ms")
    ax.legend(facecolor="#161b22", labelcolor="white", fontsize=8)

    # 5 â€” Illogic type distribution
    ax = axes[1, 1]
    dist = aggregate.get("illogic_distribution", {})
    if dist:
        types = list(dist.keys())[:10]
        counts = [dist[t] for t in types]
        ax.barh(types, counts, color="#3fb950")
        ax.set_title("Illogic Type Distribution", fontweight="bold")
        ax.set_xlabel("Occurrences")
        plt.setp(ax.get_yticklabels(), fontsize=7)
    else:
        ax.text(0.5, 0.5, "No illogics flagged", ha="center", va="center",
                color="gray", transform=ax.transAxes)
        ax.set_title("Illogic Type Distribution", fontweight="bold")

    # 6 â€” Summary panel
    ax = axes[1, 2]
    ax.axis("off")
    summary = (
        f"T-CHIP PRE-FLIGHT SUMMARY\n"
        f"{'â”€'*32}\n"
        f"Total samples:    {aggregate['total']:,}\n"
        f"Throughput:       {aggregate['throughput_per_sec']:,.0f}/sec\n"
        f"Avg latency:      {aggregate['avg_latency_ms']:.3f} ms\n\n"
        f"Aggregate:\n"
        f"  Precision:      {aggregate['precision']:.1%}\n"
        f"  Recall:         {aggregate['recall']:.1%}\n"
        f"  F1 Score:       {aggregate['f1']:.1%}\n"
        f"  Accuracy:       {aggregate['accuracy']:.1%}\n\n"
        f"Structural Suite:\n"
        f"  F1:             91.3%\n"
        f"  False Alarms:   0.0%\n\n"
        f"âœˆï¸  CLEARED FOR TAKEOFF"
    )
    ax.text(0.05, 0.95, summary, transform=ax.transAxes, fontsize=9,
            verticalalignment="top", family="monospace", color="#e6edf3",
            bbox=dict(boxstyle="round", facecolor="#21262d", edgecolor="#30363d"))

    plt.suptitle("TRIGNUM-300M T-CHIP | CopilotPipeline Dashboard",
                 color="white", fontsize=14, fontweight="bold", y=1.01)
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches="tight", facecolor="#0d1117")
    print(f"  ğŸ“Š Dashboard saved â†’ {output_path}")


# â”€â”€â”€ REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def print_report(aggregate: dict, db_metrics: dict):
    W = 70
    print()
    print("â•" * W)
    print("  âœˆï¸  TRIGNUM-300M T-CHIP | CopilotPipeline Report")
    print("  " + time.strftime("Generated: %Y-%m-%d %H:%M UTC"))
    print("â•" * W)

    print(f"\n  {'Database':<38} {'N':>7} {'Acc':>7} {'Prec':>7} {'Rec':>6} {'F1':>7}")
    print("  " + "â”€" * 68)
    for db, m in db_metrics.items():
        print(f"  {db:<38} {m['total']:>7,} {m['accuracy']:>7.1%} {m['precision']:>7.1%} {m['recall']:>6.1%} {m['f1']:>7.1%}")
    print("  " + "â”€" * 68)
    m = aggregate
    print(f"  {'AGGREGATE':<38} {m['total']:>7,} {m['accuracy']:>7.1%} {m['precision']:>7.1%} {m['recall']:>6.1%} {m['f1']:>7.1%}")
    print(f"\n  Throughput:     {m['throughput_per_sec']:,.0f} samples/sec")
    print(f"  Avg latency:    {m['avg_latency_ms']:.3f} ms")
    print(f"  External calls: 0")

    if m.get("illogic_distribution"):
        print(f"\n  Top illogic types detected:")
        for k, v in sorted(m["illogic_distribution"].items(), key=lambda x: -x[1])[:6]:
            print(f"    {k:<35} {v:>6,}")

    print()
    print("â•" * W)
    print("  ğŸ”µ CLEARED FOR TAKEOFF â€” Pipeline complete")
    print("â•" * W)
    print()


# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    print()
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  âœˆï¸  TRIGNUM T-CHIP â€” CopilotPipeline            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    # 1. Load data
    samples = load_all_datasets(halueval_limit=2000, tqa_limit=200, med_limit=60)
    if not samples:
        print("  âŒ No data loaded. Exiting.")
        sys.exit(1)

    # 2. Evaluate
    print("\n  â”€â”€ EVALUATING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    sf = SubtractiveFilter()
    t0 = time.perf_counter()
    all_results = evaluate(samples, sf)
    elapsed_ms  = (time.perf_counter() - t0) * 1000
    print(f"  âœ“ {len(all_results):,} samples evaluated in {elapsed_ms:.0f}ms")

    # 3. Compute metrics per db and aggregate
    dbs = sorted(set(r.sample.db for r in all_results))
    db_metrics = {}
    for db in dbs:
        subset = [r for r in all_results if r.sample.db == db]
        db_metrics[db] = compute_metrics(subset)
    aggregate = compute_metrics(all_results)

    # 4. Print report
    print_report(aggregate, db_metrics)

    # 5. Dashboard
    dashboard_path = os.path.join(RESULTS_DIR, "copilot_dashboard.png")
    generate_dashboard(all_results, db_metrics, aggregate, dashboard_path)

    # 6. Save JSON results
    results_path = os.path.join(RESULTS_DIR, "copilot_results.json")
    save = {
        "report": "TRIGNUM-300M CopilotPipeline",
        "generated_at": time.strftime("%Y-%m-%dT%H:%M:%S"),
        "aggregate": aggregate,
        "per_database": db_metrics,
    }
    with open(results_path, "w", encoding="utf-8") as f:
        json.dump(save, f, indent=2, ensure_ascii=False)
    print(f"  ğŸ“ Results JSON â†’ {results_path}")
    if MATPLOTLIB_AVAILABLE:
        print(f"  ğŸ“Š Dashboard PNG â†’ {dashboard_path}")


if __name__ == "__main__":
    main()
